{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "db2e5a12",
   "metadata": {},
   "source": [
    "Starting notebook for this project. Instantiate and run Policy Gradient on CartPole environment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7f72a1e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import gym \n",
    "#from gym import wrappers\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1215e020",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class PolicyNetwork(nn.Module):\n",
    "    def __init__(self,lr,input_dims,fc1_dims,fc2_dims,n_actions):\n",
    "        super(PolicyNetwork,self).__init__()\n",
    "        self.input_dims = input_dims\n",
    "        self.lr = lr\n",
    "        self.fc1_dims = fc1_dims\n",
    "        self.fc2_dims = fc2_dims\n",
    "        self.n_actions = n_actions\n",
    "        \n",
    "        self.fc1 = nn.Linear(*self.input_dims, self.fc1_dims)\n",
    "        self.fc2 = nn.Linear(self.fc1_dims,self.fc2_dims)\n",
    "        self.fc3 = nn.Linear(self.fc2_dims,self.n_actions)\n",
    "        \n",
    "        self.optimizer = torch.optim.Adam(self.parameters(),lr=lr)\n",
    "        \n",
    "        self.device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "        print(\"Device: \", self.device)\n",
    "        self.to(self.device)\n",
    "        \n",
    "    def forward(self,observation):\n",
    "        state = torch.Tensor(observation).to(self.device)\n",
    "        \n",
    "        x = F.relu(self.fc1(state))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x) #no activation, handled later\n",
    "        \n",
    "        return x\n",
    "\n",
    "\n",
    "class Agent(object):\n",
    "    #contains policy network and more!\n",
    "    def __init__(self, lr, input_dims, gamma = 0.99, n_actions = 4, l1_size = 256, l2_size = 256,batch_size = 32):\n",
    "    \n",
    "        self.gamma = gamma\n",
    "        self.reward_memory = [] #way of keeping track of rewards \n",
    "        self.action_memory = [] # and actions the agent took\n",
    "        self.batch_action_memory = [] #way to keep track or rewards and action lists inside a list of length batch size\n",
    "        self.batch_reward_memory = []\n",
    "        self.batch_size = batch_size\n",
    "        self.policy = PolicyNetwork(lr, input_dims, l1_size, l2_size, n_actions) #probability distribution used by the agent to select actions, given an observation/state. \n",
    "    \n",
    "    def choose_action(self, observation):\n",
    "        probabilities = F.softmax(self.policy.forward(observation))\n",
    "        \n",
    "        #now calculate an actual distribution from this. \n",
    "        \n",
    "        action_probs = torch.distributions.Categorical(probabilities) # probability distribuition dictated by policy network\n",
    "        \n",
    "        # now we pick an action using sample method.\n",
    "        \n",
    "        action = action_probs.sample()\n",
    "        \n",
    "        #keep track of action log probability... log pi!  You know this!\n",
    "        \n",
    "        log_probs = action_probs.log_prob(action) #clearly this is a Pytorch specific method of how to select. \n",
    "        #looks like it takes distribution, and calculates log probability of that specific action. cool. \n",
    "        \n",
    "        \n",
    "        #now save. \n",
    "        \n",
    "        self.action_memory.append(log_probs) #save a step in an episode. \n",
    "        \n",
    "        #return an action\n",
    "        \n",
    "        return action.item() #this item is an integer, now able to get fed into gym env. \n",
    "    \n",
    "    def store_rewards(self,reward):\n",
    "        self.reward_memory.append(reward) #why not just do this in main? idk, it's a little more convenient. \n",
    "        \n",
    "        \n",
    "    def learn(self):\n",
    "        #heart of the problem. Now for doing it with an entire batch..\n",
    "        \n",
    "        \n",
    "        self.policy.optimizer.zero_grad()\n",
    "        \n",
    "        \n",
    "        \n",
    "        #It learns at the end of the episode, which is not a good thing! Would want an average of future returns, not per ep. \n",
    "        Gs = []\n",
    "        for episode in self.batch_reward_memory:\n",
    "            \n",
    "            G = np.zeros_like(self.reward_memory,dtype = np.float64) #For mc Reinforce\n",
    "            for t in range(len(self.reward_memory)):\n",
    "                G_sum = 0\n",
    "                discount = 1 \n",
    "                for k in range(t,len(self.reward_memory)): #why from t? Rewards to go! \n",
    "                    G_sum += self.reward_memory[k] * discount\n",
    "                    discount *= self.gamma  # decreases for future time steps. \n",
    "                \n",
    "                G[t] = G_sum  # at the end of episode, store sum of returns at timestep t\n",
    "            \n",
    "            #standardize to reduce variance. Free lunch! \n",
    "            mean = np.mean(G)\n",
    "            std = np.std(G) if np.std(G) > 0 else 1\n",
    "\n",
    "            G = (G - mean)/std\n",
    "            G = torch.Tensor(G).to(self.policy.device) #for some reason, worked like this. Phil had to use a specific data type. \n",
    "            Gs.append(G)\n",
    "        \n",
    "        \n",
    "        loss = 0\n",
    "        for G, action_memory in zip(Gs, self.batch_action_memory):\n",
    "            for g, logprob in zip(G, action_memory):\n",
    "                loss += -g * logprob #weight each probbility by future + current reward at that timestpe\n",
    "\n",
    "                    #it's objective is to maximize this prbabil8ty \n",
    "            #spoilers. To do this for multiple trajectories, need to iterate over N episodes, and average these losses. \n",
    "            \n",
    "            #another spoiler. Baseline should be subjtracted from here, perhaps just as mean G? \n",
    "            \n",
    "            # another spoiler. Critic also goes here too! \n",
    "        #backprop!\n",
    "        loss = loss / self.batch_size # so it's an average. \n",
    "        \n",
    "        loss.backward()\n",
    "        self.policy.optimizer.step()\n",
    "\n",
    "        #zero out and repeat. This is a Sample inefficient MC, and a future improvement \n",
    "        self.action_memory = []\n",
    "        self.reward_memory = []\n",
    "        self.batch_action_memory = []\n",
    "        self.batch_reward_memory = []\n",
    "\n",
    "            #now for the main!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "885751c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device:  cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:43: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode :  0 score  15.0\n",
      "episode :  1 score  47.0\n",
      "episode :  2 score  13.0\n",
      "episode :  3 score  25.0\n",
      "episode :  4 score  17.0\n",
      "episode :  5 score  14.0\n",
      "episode :  6 score  8.0\n",
      "episode :  7 score  16.0\n",
      "episode :  8 score  18.0\n",
      "episode :  9 score  26.0\n",
      "episode :  10 score  75.0\n",
      "episode :  11 score  41.0\n",
      "episode :  12 score  32.0\n",
      "episode :  13 score  34.0\n",
      "episode :  14 score  75.0\n",
      "episode :  15 score  38.0\n",
      "episode :  16 score  19.0\n",
      "episode :  17 score  43.0\n",
      "episode :  18 score  42.0\n",
      "episode :  19 score  64.0\n",
      "episode :  20 score  27.0\n",
      "episode :  21 score  88.0\n",
      "episode :  22 score  19.0\n",
      "episode :  23 score  45.0\n",
      "episode :  24 score  52.0\n",
      "episode :  25 score  17.0\n",
      "episode :  26 score  18.0\n",
      "episode :  27 score  35.0\n",
      "episode :  28 score  19.0\n",
      "episode :  29 score  14.0\n",
      "episode :  30 score  35.0\n",
      "episode :  31 score  24.0\n",
      "episode :  32 score  14.0\n",
      "episode :  33 score  28.0\n",
      "episode :  34 score  20.0\n",
      "episode :  35 score  12.0\n",
      "episode :  36 score  60.0\n",
      "episode :  37 score  27.0\n",
      "episode :  38 score  34.0\n",
      "episode :  39 score  28.0\n",
      "episode :  40 score  14.0\n",
      "episode :  41 score  70.0\n",
      "episode :  42 score  14.0\n",
      "episode :  43 score  18.0\n",
      "episode :  44 score  59.0\n",
      "episode :  45 score  25.0\n",
      "episode :  46 score  16.0\n",
      "episode :  47 score  20.0\n",
      "episode :  48 score  28.0\n",
      "episode :  49 score  17.0\n",
      "episode :  50 score  26.0\n",
      "episode :  51 score  39.0\n",
      "episode :  52 score  55.0\n",
      "episode :  53 score  18.0\n",
      "episode :  54 score  22.0\n",
      "episode :  55 score  20.0\n",
      "episode :  56 score  27.0\n",
      "episode :  57 score  14.0\n",
      "episode :  58 score  33.0\n",
      "episode :  59 score  34.0\n",
      "episode :  60 score  15.0\n",
      "episode :  61 score  21.0\n",
      "episode :  62 score  14.0\n",
      "episode :  63 score  20.0\n",
      "episode :  64 score  20.0\n",
      "episode :  65 score  16.0\n",
      "episode :  66 score  50.0\n",
      "episode :  67 score  23.0\n",
      "episode :  68 score  26.0\n",
      "episode :  69 score  19.0\n",
      "episode :  70 score  56.0\n",
      "episode :  71 score  36.0\n",
      "episode :  72 score  33.0\n",
      "episode :  73 score  74.0\n",
      "episode :  74 score  59.0\n",
      "episode :  75 score  82.0\n",
      "episode :  76 score  29.0\n",
      "episode :  77 score  32.0\n",
      "episode :  78 score  109.0\n",
      "episode :  79 score  45.0\n",
      "episode :  80 score  47.0\n",
      "episode :  81 score  58.0\n",
      "episode :  82 score  31.0\n",
      "episode :  83 score  72.0\n",
      "episode :  84 score  51.0\n",
      "episode :  85 score  26.0\n",
      "episode :  86 score  138.0\n",
      "episode :  87 score  58.0\n",
      "episode :  88 score  22.0\n",
      "episode :  89 score  68.0\n",
      "episode :  90 score  81.0\n",
      "episode :  91 score  37.0\n",
      "episode :  92 score  53.0\n",
      "episode :  93 score  27.0\n",
      "episode :  94 score  56.0\n",
      "episode :  95 score  49.0\n",
      "episode :  96 score  149.0\n",
      "episode :  97 score  113.0\n",
      "episode :  98 score  23.0\n",
      "episode :  99 score  50.0\n",
      "episode :  100 score  46.0\n",
      "episode :  101 score  154.0\n",
      "episode :  102 score  79.0\n",
      "episode :  103 score  99.0\n",
      "episode :  104 score  76.0\n",
      "episode :  105 score  84.0\n",
      "episode :  106 score  132.0\n",
      "episode :  107 score  96.0\n",
      "episode :  108 score  157.0\n",
      "episode :  109 score  152.0\n",
      "episode :  110 score  130.0\n",
      "episode :  111 score  51.0\n",
      "episode :  112 score  22.0\n",
      "episode :  113 score  121.0\n",
      "episode :  114 score  174.0\n",
      "episode :  115 score  143.0\n",
      "episode :  116 score  69.0\n",
      "episode :  117 score  93.0\n",
      "episode :  118 score  92.0\n",
      "episode :  119 score  176.0\n",
      "episode :  120 score  164.0\n",
      "episode :  121 score  145.0\n",
      "episode :  122 score  200.0\n",
      "episode :  123 score  200.0\n",
      "episode :  124 score  200.0\n",
      "episode :  125 score  200.0\n",
      "episode :  126 score  200.0\n",
      "episode :  127 score  177.0\n",
      "episode :  128 score  155.0\n",
      "episode :  129 score  111.0\n",
      "episode :  130 score  126.0\n",
      "episode :  131 score  150.0\n",
      "episode :  132 score  85.0\n",
      "episode :  133 score  74.0\n",
      "episode :  134 score  122.0\n",
      "episode :  135 score  93.0\n",
      "episode :  136 score  121.0\n",
      "episode :  137 score  68.0\n",
      "episode :  138 score  113.0\n",
      "episode :  139 score  144.0\n",
      "episode :  140 score  184.0\n",
      "episode :  141 score  126.0\n",
      "episode :  142 score  131.0\n",
      "episode :  143 score  159.0\n",
      "episode :  144 score  193.0\n",
      "episode :  145 score  181.0\n",
      "episode :  146 score  158.0\n",
      "episode :  147 score  88.0\n",
      "episode :  148 score  66.0\n",
      "episode :  149 score  113.0\n",
      "episode :  150 score  131.0\n",
      "episode :  151 score  44.0\n",
      "episode :  152 score  85.0\n",
      "episode :  153 score  119.0\n",
      "episode :  154 score  49.0\n",
      "episode :  155 score  99.0\n",
      "episode :  156 score  83.0\n",
      "episode :  157 score  105.0\n",
      "episode :  158 score  118.0\n",
      "episode :  159 score  124.0\n",
      "episode :  160 score  111.0\n",
      "episode :  161 score  179.0\n",
      "episode :  162 score  156.0\n",
      "episode :  163 score  147.0\n",
      "episode :  164 score  135.0\n",
      "episode :  165 score  141.0\n",
      "episode :  166 score  160.0\n",
      "episode :  167 score  167.0\n",
      "episode :  168 score  140.0\n",
      "episode :  169 score  200.0\n",
      "episode :  170 score  200.0\n",
      "episode :  171 score  200.0\n",
      "episode :  172 score  200.0\n",
      "episode :  173 score  200.0\n",
      "episode :  174 score  200.0\n",
      "episode :  175 score  173.0\n",
      "episode :  176 score  200.0\n",
      "episode :  177 score  168.0\n",
      "episode :  178 score  200.0\n",
      "episode :  179 score  200.0\n",
      "episode :  180 score  200.0\n",
      "episode :  181 score  200.0\n",
      "episode :  182 score  200.0\n",
      "episode :  183 score  197.0\n",
      "episode :  184 score  200.0\n",
      "episode :  185 score  200.0\n",
      "episode :  186 score  200.0\n",
      "episode :  187 score  200.0\n",
      "episode :  188 score  172.0\n",
      "episode :  189 score  142.0\n",
      "episode :  190 score  140.0\n",
      "episode :  191 score  137.0\n",
      "episode :  192 score  151.0\n",
      "episode :  193 score  140.0\n",
      "episode :  194 score  159.0\n",
      "episode :  195 score  200.0\n",
      "episode :  196 score  165.0\n",
      "episode :  197 score  196.0\n",
      "episode :  198 score  200.0\n",
      "episode :  199 score  200.0\n",
      "episode :  200 score  200.0\n",
      "episode :  201 score  200.0\n",
      "episode :  202 score  200.0\n",
      "episode :  203 score  200.0\n",
      "episode :  204 score  200.0\n",
      "episode :  205 score  200.0\n",
      "episode :  206 score  200.0\n",
      "episode :  207 score  200.0\n",
      "episode :  208 score  184.0\n",
      "episode :  209 score  160.0\n",
      "episode :  210 score  140.0\n",
      "episode :  211 score  87.0\n",
      "episode :  212 score  128.0\n",
      "episode :  213 score  115.0\n",
      "episode :  214 score  86.0\n",
      "episode :  215 score  105.0\n",
      "episode :  216 score  108.0\n",
      "episode :  217 score  106.0\n",
      "episode :  218 score  94.0\n",
      "episode :  219 score  101.0\n",
      "episode :  220 score  83.0\n",
      "episode :  221 score  105.0\n",
      "episode :  222 score  92.0\n",
      "episode :  223 score  36.0\n",
      "episode :  224 score  48.0\n",
      "episode :  225 score  31.0\n",
      "episode :  226 score  27.0\n",
      "episode :  227 score  36.0\n",
      "episode :  228 score  24.0\n",
      "episode :  229 score  48.0\n",
      "episode :  230 score  45.0\n",
      "episode :  231 score  22.0\n",
      "episode :  232 score  63.0\n",
      "episode :  233 score  39.0\n",
      "episode :  234 score  59.0\n",
      "episode :  235 score  42.0\n",
      "episode :  236 score  45.0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-a4205fac442c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m            \u001b[0;31m# env.render()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m             \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoose_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m             \u001b[0mobservation_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstore_rewards\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreward\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-899f9ee7a323>\u001b[0m in \u001b[0;36mchoose_action\u001b[0;34m(self, observation)\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0;31m#keep track of action log probability... log pi!  You know this!\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m         \u001b[0mlog_probs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maction_probs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_prob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#clearly this is a Pytorch specific method of how to select.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m         \u001b[0;31m#looks like it takes distribution, and calculates log probability of that specific action. cool.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/distributions/categorical.py\u001b[0m in \u001b[0;36mlog_prob\u001b[0;34m(self, value)\u001b[0m\n\u001b[1;32m    115\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mlog_prob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_sample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m         \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m         \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_pmf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbroadcast_tensors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/distributions/distribution.py\u001b[0m in \u001b[0;36m_validate_sample\u001b[0;34m(self, value)\u001b[0m\n\u001b[1;32m    274\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    275\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0msupport\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 276\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msupport\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    277\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'The value argument must be within the support'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    278\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/distributions/constraints.py\u001b[0m in \u001b[0;36mcheck\u001b[0;34m(self, value)\u001b[0m\n\u001b[1;32m    248\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcheck\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 250\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m&\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower_bound\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m&\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupper_bound\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    251\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "#Main \n",
    "score_history = []\n",
    "score = 0\n",
    "n_episodes = 800\n",
    "batch_size = 1\n",
    "#env = gym.make('LunarLander-v2')\n",
    "env = gym.make('CartPole-v0')\n",
    "\n",
    "agent = Agent(lr = 0.001, input_dims=[4],gamma=0.99,n_actions=env.action_space.n\n",
    "             ,l1_size=128,l2_size=128,batch_size=batch_size)\n",
    "\n",
    "\n",
    "\n",
    "#env = wrappers.Monitor(env, 'tmp/lunar-lander',\n",
    " #                      video_callable=lambda episodeid: True, force=True)\n",
    "\n",
    "for i in range(n_episodes):\n",
    "\n",
    "    for _ in range(batch_size):\n",
    "        done = False\n",
    "        score = 0\n",
    "        observation = env.reset()\n",
    "        while not done:\n",
    "           # env.render()\n",
    "            action = agent.choose_action(observation)\n",
    "            observation_,reward, done, info = env.step(action)\n",
    "            agent.store_rewards(reward)\n",
    "            observation = observation_ #set the old obvs to the new one\n",
    "            score += reward\n",
    "        agent.batch_action_memory.append(agent.action_memory)\n",
    "        agent.batch_reward_memory.append(agent.reward_memory)\n",
    "    \n",
    "    score_history.append(score)\n",
    "    agent.learn()\n",
    "    print('episode : ', i, 'score ', score)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "74adf464",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4,)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "529151e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def show_video():\n",
    "    mp4list = glob.glob('video/*.mp4')\n",
    "    if len(mp4list) > 0:\n",
    "        mp4 = mp4list[0]\n",
    "        video = io.open(mp4, 'r+b').read()\n",
    "        encoded = base64.b64encode(video)\n",
    "        ipythondisplay.display(HTML(data='''<video alt=\"test\" autoplay \n",
    "                loop controls style=\"height: 400px;\">\n",
    "                <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n",
    "             </video>'''.format(encoded.decode('ascii'))))\n",
    "    else: \n",
    "        print(\"Could not find video\")\n",
    "\n",
    "\n",
    "def wrap_env(env):\n",
    "    env = Monitor(env, './video', force=True)\n",
    "    return env\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e991de94",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
